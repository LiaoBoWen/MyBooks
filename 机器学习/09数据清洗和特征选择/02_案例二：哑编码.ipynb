{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "- [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)\n",
    "- [FeatureHasher](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先了解机器学习中的特征类别：连续型特征和离散型特征     \n",
    "\n",
    "       拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1].如果使用logistic回归，w1*x1+w2*x2，因为x1的取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。\n",
    "\n",
    "       对于连续性特征：\n",
    "\n",
    "Rescale bounded continuous features: All continuous input that are bounded, rescale them to [-1, 1] through x = (2x - max - min)/(max - min).    线性放缩到[-1,1]\n",
    "\n",
    "Standardize all continuous features: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean (u) and standard deviation (s) and do x = (x - u)/s.       放缩到均值为0，方差为1\n",
    "\n",
    "       对于离散性特征：\n",
    "\n",
    "Binarize categorical/discrete features: 对于离散的特征基本就是按照one-hot（独热）编码，该离散特征有多少取值，就用多少维来表示该特征。\n",
    " \n",
    "\n",
    "一. 什么是独热编码？\n",
    "\n",
    "       独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：\n",
    "\n",
    "       假如有三种颜色特征：红、黄、蓝。 在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3. 那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红<黄<蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。\n",
    "\n",
    "      自然状态码为：000,001,010,011,100,101\n",
    "\n",
    "      独热编码为：000001,000010,000100,001000,010000,100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer,FeatureHasher\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 3]\n",
      " [1 1 0]\n",
      " [0 2 1]\n",
      " [1 0 2]]\n",
      "[2 3 4]\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2],[1,1,1]]) \n",
    "a=np.array([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "print(a)\n",
    "# 基于列进行编码\n",
    "print(enc.n_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把0放到第一列[0,1]比对，把1放到第二列[0,1,2]比对，把3放到第三列[0,1,2，3]比对\n",
    "enc.transform([[0, 1, 3]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二. 为什么要独热编码？\n",
    "\n",
    "      正如上文所言，独热编码（哑变量 dummy variable）是因为大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。       \n",
    "\n",
    "        为什么特征向量要映射到欧式空间？\n",
    "\n",
    "        将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。\n",
    "\n",
    " \n",
    "\n",
    "三 .独热编码优缺点\n",
    "\n",
    "优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。\n",
    "\n",
    "缺点：当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。\n",
    " \n",
    "\n",
    "四. 什么情况下(不)用独热编码？\n",
    "\n",
    "用：独热编码用来解决类别型数据的离散值问题，\n",
    "不用：将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。  Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。\n",
    "总的来说，要是one hot encoding的类别数目不太多，建议优先考虑。 \n",
    "\n",
    " \n",
    "\n",
    "五.  什么情况下(不)需要归一化？\n",
    "\n",
    "需要： 基于参数的模型或基于距离的模型，都是要进行特征的归一化。\n",
    "不需要：基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。\n",
    " \n",
    "\n",
    "六.  标签编码LabelEncoder\n",
    "\n",
    "作用： 利用LabelEncoder() 将转换成连续的数值型变量。即是对不连续的数字或者文本进行编号例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit([1,5,67,100])\n",
    "le.transform([1,1,100,67,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "le.fit_transform([1,5,67,100])\n",
    "print(le.transform([1,1,100,67,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder()\n",
      "['amsterdam', 'paris', 'tokyo']\n",
      "[2 2 1]\n",
      "['tokyo', 'tokyo', 'paris']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TOOL\\zwPython\\py36\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]))\n",
    "print(list(le.classes_))\n",
    "print(le.transform([\"tokyo\", \"tokyo\", \"paris\"]))\n",
    "print(list(le.inverse_transform([2, 2, 1])))   # 逆过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding在某些情况下很有用，但是场景限制很多。再举一例：比如有[dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat。所以目前还没有发现标签编码的广泛使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 0. 1.]\n",
      " [0. 2. 3. 0.]]\n",
      "['bar', 'baz', 'foa', 'foo']\n",
      "[[ 0. 10.  0.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# 将特征值映射列表转换为向量\n",
    "dv = DictVectorizer(sparse=False)\n",
    "D = [{'foo':1, 'bar':2}, {'foa':3, 'baz': 2}]\n",
    "X = dv.fit_transform(D)\n",
    "print (X)\n",
    "print (dv.get_feature_names())\n",
    "print (dv.transform({'foo':4, 'unseen':3,'baz':10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TOOL\\zwPython\\py36\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4., 1., 0., 0., 0., 0., 0., 2.],\n",
       "       [0., 0., 0., 2., 5., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hash转换\n",
    "h = FeatureHasher(n_features=10,non_negative=True)\n",
    "D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n",
    "f = h.fit_transform(D)\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
